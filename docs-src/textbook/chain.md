
# Chain Replication 

As you have probably experienced, computers can crash. If you are
running a web service, you may not be able to afford a long outage. If
you are running software that flies a plane, then an outage for any
length of time could lead to a disaster. To deal with service outages
caused by computers crashing, you may want to *replicate* the service
onto multiple computers. As long as one of the computers survives, the
service remains available.

Besides availability, it is usually important that the replicated
service acts as if it were a single one. This requires that the replicas
of the service coordinate their actions. The *Replicated State Machine
Approach* is a general approach to do just this. First,
you model your service as a deterministic state machine. The replicas
each run a copy of the state machine, started in the same state. As long
as the replicas handle the same inputs in the same order, determinism
guarantees that they produce the same outputs in the same order.

```python title="rsm.hny"
--8<-- "rsm.hny"
```

<figcaption>Figure 27.1 (<a href=https://harmony.cs.cornell.edu/code/rsm.hny>code/rsm.hny</a>): 
Replicated State Machine </figcaption>

![](figures/rsmspec.png)
<figcaption>Figure 27.2: The DFA generated by Figure 27.1 when `NOPS=2`
and `NREPLICAS=2`</figcaption>

Figure 27.1 presents a Harmony specification of state machine
replication. We model the state machine as a *history*: a sequence of
operations. In a replicated state machine, the abstract network maintains this
history as an ordered queue of messages. `NOPS` clients each place an
operation on the network. The replicas process messages from the ordered network.

All but one of the replicas is allowed to crash. Crashes are modeled as
interrupts, so we use Harmony's `trap` clause to schedule one.  When
crashing, a replica simply stops. The model chooses one of the replicas
that is not allowed to crash. Of course, a replica does not know whether
it is immortal or not in practice---it should just assume that it is. The
immortality of one of the replicas is only used for modeling the assumptions
we make about the system.

The behavior is captured as before.  Before an operation is added to
the network, a client prints the operation (in this case, its own
identifier). After a replica processes an operation, it prints a
pair consisting of its own identifier and the operation. All replicas
print the same operations in the same order until they crash.
Figure 27.2 shows the allowed behaviors in case there are just
two clients and two replicas.  Because one of the replicas is immortal and
clients do not crash, at least one of the replicas will print both
operations (liveness).  If both do, they do so in the same order (safety).

But in reality the network is not an ordered queue and better modeled as
a set of messages. The trick now is to ensure that all replicas handle
the same requests in the same order and to do so in a way that continues
to work even if some strict subset of replicas crash. *Chain
Replication* is such a replication protocol. In Chain Replication, the
replicas are organized in a linear chain which may change as replicas crash.
Importantly, at any point in time there is only one *head* and
one *tail* replica.

Only the head is allowed to accept new operations from clients.
When it does so, it adds the operation to the end of its history and sends the
history to its successor on the chain. When the direct successor
receives such a history, it makes sure that the history is an extension
of its own and, if so, replaces its own history with the one received.
It then sends the history on to its successor, if any.

When an operation reaches the tail, the operation is what is known as
*stable* --- it has been reliably ordered and persisted.

So, when a replica fails, its successors should find out about it.
In order for this to work, each replica needs to know who is its
predecessor and who is its successor. So, when a replica fails, 
its neighbors should find out about it. In practice, one server can
detect the failure of another server by pinging it. If a server does
not receive a response to its ping within some maximum amount of time,
then the server considers its peer crashed. Note that this, in general,
is not a safe thing to do---the network or the peer may be temporarily
slow but the peer is not necessarily crashed when the timer
expires.

Nonetheless, we will assume here that failure detection
does not make mistakes and that eventually every failure is detected.
This is called the *Fail-Stop* failure model, which is distinct from
the often more realistic *Crash Failure* model where processes can
crash but accurate detection is not available. We will consider that
more realistic failure model in the upcoming chapters.

```python title="chain.hny"
--8<-- "chain.hny"
```

<figcaption>Figure 27.3 (<a href=https://harmony.cs.cornell.edu/code/chain.hny>code/chain.hny</a>): 
Chain Replication </figcaption>

Figure 27.3 show an implemenation of chain replication.
The network is modeled as a append-only set of messages of the form
`(destination, (source, payload))`.  When sending, a message is
atomically added to this set. A client broadcasts its operation to
all replicas.

Each replica maintains its own history `hist` and a chain
configuration `config`.  The replica executes a loop in which it
receives and atomically handles messages until it crashes.  As before,
one of the replicas cannot crash. Because replicas do not want to
handle the same message twice, each replica maintains a set `received`
of messages it has already handled.  Each replica then waits for a
message on the network it has not already handled before.

When a replica receives a client request, it adds the request to a
set `requests` that it maintains.  A replica can only handle such a
request if it is the head, but each replica can eventually become the
head so it should carefully save all requests.  (In theory, it can
remove them as soon as they are on its history.) When a replica receives
a failure notification, it updates its configuration accordingly. When
a non-head replica receives a history that extends its own history, then
the replica adopts the received history.

Next, if a replica is the head, it adds any requests it has received
to its history unless they are already on there. If a replica is the
tail, it "delivers" operations on its history (by printing the operation)
that it has not already delivered. For this, it maintains a counter
`delivered` that counts the number of delivered requests. Any replica
that is not the tail sends its history to its successor in the chain.

The question is whether chain replication has the same behavior as the
replicated state machine specification of Figure 27.1. This can be
checked using the following two Harmony commands:

    $ harmony -o rsm.hfa code/rsm.hny
    $ harmony -B rsm.hfa code/chain.hny

The first command outputs the behavior DFA of `code/rsm.hny` in the
file `rsm.hfa`. The second command checks that the behavior of
`code/chain.hny` satisfies the DFA in `rsm.hfa`. Note that chain
replication does not produce all the possible behaviors of a replicated
state machine, but all its behaviors are valid.

The model has each replica send its entire history each time it extends
its history. This is fine for modeling, but in practice that would not
scale. In practice, a predecessor would set up a TCP connection to its
successor and only send updates to its history along the TCP connection.
Because TCP connections guarantee FIFO order, this would be identical to
the predecessor sending a series of histories, but much more efficient.
